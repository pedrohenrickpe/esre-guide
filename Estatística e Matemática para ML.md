# üéØ OBJETIVO FINAL

Ao final do plano, ser  capaz de:

* Entender matematicamente o funcionamento de embeddings, aten√ß√£o e similaridade sem√¢ntica.
* Ler papers de NLP e LLMs com compreens√£o real (n√£o s√≥ reproduzir c√≥digo).
* Implementar do zero pequenos modelos de linguagem e sistemas de busca vetorial.

---

# üóìÔ∏è PLANO DE ESTUDO (aprox. 6 meses)

Cada m√≥dulo dura **3 a 4 semanas** 

---

## üîπ M√≥dulo 1 ‚Äì Fundamentos de Matem√°tica e Estat√≠stica (4 semanas)

### üìò T√≥picos

* √Ålgebra Linear:

  * Vetores, matrizes, produto escalar, norma, proje√ß√µes.
  * Autovalores, decomposi√ß√£o SVD e PCA.
* Probabilidade e Estat√≠stica:

  * Probabilidade condicional e regra de Bayes.
  * Vari√°vel aleat√≥ria, m√©dia, vari√¢ncia, covari√¢ncia.
  * Entropia e distribui√ß√£o de probabilidade.

### üíª Pr√°tica

* Manipular vetores/matrizes com **NumPy**.
* Calcular **cosine similarity** entre embeddings.
* Fazer **PCA e visualiza√ß√£o de embeddings** com `scikit-learn`.

### üìö Recursos sugeridos

* Livro: *Matem√°tica Essencial para Machine Learning* (Marc Peter Deisenroth et al.)
* Curso: [Khan Academy ‚Äì √Ålgebra Linear e Probabilidade](https://pt.khanacademy.org/)
* Curso pr√°tico: [Linear Algebra for Machine Learning (Coursera)](https://www.coursera.org/learn/linear-algebra-machine-learning)

---

## üîπ M√≥dulo 2 ‚Äì C√°lculo e Otimiza√ß√£o (3 semanas)

### üìò T√≥picos

* Derivadas e gradientes.
* Fun√ß√µes convexas e n√£o convexas.
* Descida do gradiente (SGD, Adam).
* Fun√ß√µes de ativa√ß√£o e perda (ReLU, softmax, cross-entropy).

### üíª Pr√°tica

* Implementar **gradient descent** do zero em Python.
* Treinar um modelo simples (regress√£o linear/log√≠stica).
* Visualizar superf√≠cies de perda e gradientes.

### üìö Recursos sugeridos

* Livro: *Deep Learning* (Goodfellow, Bengio, Courville) ‚Äì cap√≠tulos 4 e 8.
* Curso: [Andrew Ng ‚Äì Machine Learning (Coursera)](https://www.coursera.org/learn/machine-learning)
* Playground interativo: [https://playground.tensorflow.org/](https://playground.tensorflow.org/)

---

## üîπ M√≥dulo 3 ‚Äì Fundamentos de Machine Learning (4 semanas)

### üìò T√≥picos

* Regress√£o linear e log√≠stica.
* Overfitting, underfitting e regulariza√ß√£o.
* Redes neurais feed-forward e backpropagation.
* Fun√ß√µes de perda e otimiza√ß√£o.
* M√©tricas e avalia√ß√£o.

### üíª Pr√°tica

* Implementar rede neural simples com **PyTorch**.
* Visualizar o impacto da regulariza√ß√£o.
* Aplicar modelos em datasets pequenos (Iris, MNIST).

### üìö Recursos sugeridos

* Livro: *Hands-On Machine Learning with Scikit-Learn & TensorFlow* (Aur√©lien G√©ron).
* Curso: [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)

---

## üîπ M√≥dulo 4 ‚Äì Teoria da Informa√ß√£o e Linguagem (3 semanas)

### üìò T√≥picos

* Entropia, perplexidade e cross-entropy loss.
* Modelos de linguagem baseados em probabilidade (n-gramas, Markov).
* Introdu√ß√£o √† Teoria da Informa√ß√£o (Shannon).
* Representa√ß√£o distribu√≠da de palavras (Word2Vec, GloVe).

### üíª Pr√°tica

* Calcular perplexidade de um modelo n-grama.
* Treinar embeddings com **gensim (Word2Vec)**.
* Visualizar palavras pr√≥ximas em espa√ßo vetorial (PCA, t-SNE).

### üìö Recursos sugeridos

* Artigo: ["Word2Vec Explained"](https://arxiv.org/abs/1402.3722)
* Curso: [CS224N ‚Äì Natural Language Processing with Deep Learning (Stanford)](http://web.stanford.edu/class/cs224n/)

---

## üîπ M√≥dulo 5 ‚Äì Redes Neurais para NLP e Transformers (5 semanas)

### üìò T√≥picos

* Embeddings contextuais e autoaten√ß√£o.
* Estrutura do Transformer (attention, residuals, layer norm).
* Masked Language Modeling (MLM).
* Fine-tuning de modelos pr√©-treinados (BERT, GPT, etc).

### üíª Pr√°tica

* Implementar a camada de **self-attention** do zero.
* Fine-tuning com **Hugging Face Transformers** (BERT, DistilBERT).
* Avaliar embeddings em tarefas sem√¢nticas.

### üìö Recursos sugeridos

* Livro: *The Illustrated Transformer* (Jay Alammar).
* Curso: [Hugging Face ‚Äì NLP Course](https://huggingface.co/learn/nlp-course)
* Paper: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

---

## üîπ M√≥dulo 6 ‚Äì Busca Sem√¢ntica e Embeddings (4 semanas)

### üìò T√≥picos

* Espa√ßos vetoriais e m√©tricas de similaridade.
* Indexa√ß√£o vetorial (FAISS, Annoy).
* Retrieval-Augmented Generation (RAG).
* Fine-tuning de embeddings com *contrastive learning*.

### üíª Pr√°tica

* Implementar um **sistema de busca sem√¢ntica** usando **Sentence Transformers + FAISS**.
* Criar um **RAG pipeline** com LangChain ou LlamaIndex.
* Avaliar resultados com *recall@k* e *MRR*.

### üìö Recursos sugeridos

* Reposit√≥rio: [Sentence Transformers](https://www.sbert.net/)
* Curso: [DeepLearning.AI ‚Äì Vector Databases and Semantic Search](https://www.deeplearning.ai/)
* Blog: [Pinecone ‚Äì Semantic Search Guide](https://www.pinecone.io/learn/semantic-search/)

---

## üîπ M√≥dulo 7 ‚Äì Projeto Final (2‚Äì3 semanas)

Monte um **projeto de portf√≥lio completo**:

> **‚ÄúBusca Sem√¢ntica com LLM + RAG‚Äù**

### Etapas:

1. Criar embeddings de documentos com `sentence-transformers`.
2. Indexar vetores com FAISS ou Milvus.
3. Implementar um *retriever + LLM* para responder perguntas com contexto.
4. Avaliar precis√£o das respostas e explicar as m√©tricas.

---

# üß≠ Resultado

Ao final do plano, dominar:

* Matem√°tica e estat√≠stica aplicadas a NLP e LLMs.
* Constru√ß√£o de sistemas de busca sem√¢ntica e RAG.
* Entendimento matem√°tico profundo de embeddings e aten√ß√£o.

---
